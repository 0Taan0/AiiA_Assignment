{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#text processing\n",
    "import nltk #nltk oder spacey\n",
    "import string\n",
    "import re #regular expression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 18:09:23.748706: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 18:09:23.787564: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-24 18:09:23.787595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-24 18:09:23.787617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 18:09:23.795360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-24 18:09:24.941981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "#model(hugging face)\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReceiverID</th>\n",
       "      <th>ActionType</th>\n",
       "      <th>NegoOutcome</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_Length</th>\n",
       "      <th>Sentence_Count</th>\n",
       "      <th>Word_Count_nltk</th>\n",
       "      <th>NegoOutcomeLabel</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentCategory</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SenderID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>69</td>\n",
       "      <td>Offer</td>\n",
       "      <td>FinalAccept</td>\n",
       "      <td>hope well management company agreed building f...</td>\n",
       "      <td>2529</td>\n",
       "      <td>17</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213699</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalAccept</td>\n",
       "      <td>reaching mei apologize slight delay getting ba...</td>\n",
       "      <td>2579</td>\n",
       "      <td>21</td>\n",
       "      <td>483</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165002</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>69</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalAccept</td>\n",
       "      <td>r kind response need hurry guess face daily fi...</td>\n",
       "      <td>2336</td>\n",
       "      <td>15</td>\n",
       "      <td>454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222533</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalAccept</td>\n",
       "      <td>swift reply read set proposal discussed collea...</td>\n",
       "      <td>1961</td>\n",
       "      <td>13</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.160333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>69</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalAccept</td>\n",
       "      <td>proposal effort far seems getting close resolv...</td>\n",
       "      <td>1917</td>\n",
       "      <td>10</td>\n",
       "      <td>376</td>\n",
       "      <td>1</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>856</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>latest offer point completely let u take corpo...</td>\n",
       "      <td>1755</td>\n",
       "      <td>21</td>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285340</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>851</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>much latest offer happy could already reached ...</td>\n",
       "      <td>1460</td>\n",
       "      <td>17</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.282807</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>856</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>meyer fast answer happy found solution issue n...</td>\n",
       "      <td>673</td>\n",
       "      <td>10</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>851</td>\n",
       "      <td>Counteroffer</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>still interested coming joint result neverthel...</td>\n",
       "      <td>969</td>\n",
       "      <td>9</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163158</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>856</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>FinalReject</td>\n",
       "      <td>sorry accept less understand argument like las...</td>\n",
       "      <td>276</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2332 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ReceiverID    ActionType  NegoOutcome  \\\n",
       "SenderID                                          \n",
       "70                69         Offer  FinalAccept   \n",
       "69                70  Counteroffer  FinalAccept   \n",
       "70                69  Counteroffer  FinalAccept   \n",
       "69                70  Counteroffer  FinalAccept   \n",
       "70                69  Counteroffer  FinalAccept   \n",
       "...              ...           ...          ...   \n",
       "851              856  Counteroffer  FinalReject   \n",
       "856              851  Counteroffer  FinalReject   \n",
       "851              856  Counteroffer  FinalReject   \n",
       "856              851  Counteroffer  FinalReject   \n",
       "851              856   FinalReject  FinalReject   \n",
       "\n",
       "                                                    Content  Content_Length  \\\n",
       "SenderID                                                                      \n",
       "70        hope well management company agreed building f...            2529   \n",
       "69        reaching mei apologize slight delay getting ba...            2579   \n",
       "70        r kind response need hurry guess face daily fi...            2336   \n",
       "69        swift reply read set proposal discussed collea...            1961   \n",
       "70        proposal effort far seems getting close resolv...            1917   \n",
       "...                                                     ...             ...   \n",
       "851       latest offer point completely let u take corpo...            1755   \n",
       "856       much latest offer happy could already reached ...            1460   \n",
       "851       meyer fast answer happy found solution issue n...             673   \n",
       "856       still interested coming joint result neverthel...             969   \n",
       "851       sorry accept less understand argument like las...             276   \n",
       "\n",
       "          Sentence_Count  Word_Count_nltk  NegoOutcomeLabel  Sentiment  \\\n",
       "SenderID                                                                 \n",
       "70                    17              470                 1   0.213699   \n",
       "69                    21              483                 1   0.165002   \n",
       "70                    15              454                 1   0.222533   \n",
       "69                    13              381                 1   0.160333   \n",
       "70                    10              376                 1   0.122500   \n",
       "...                  ...              ...               ...        ...   \n",
       "851                   21              358                 0   0.285340   \n",
       "856                   17              294                 0   0.282807   \n",
       "851                   10              144                 0   0.251786   \n",
       "856                    9              191                 0   0.163158   \n",
       "851                    5               60                 0   0.056771   \n",
       "\n",
       "         SentimentCategory  \n",
       "SenderID                    \n",
       "70                Positive  \n",
       "69                Positive  \n",
       "70                Positive  \n",
       "69                Positive  \n",
       "70                Positive  \n",
       "...                    ...  \n",
       "851               Positive  \n",
       "856               Positive  \n",
       "851               Positive  \n",
       "856               Positive  \n",
       "851               Positive  \n",
       "\n",
       "[2332 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('df_complete_cleansing.xlsx', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nego_message_data=df['Content']\n",
    "#df_nego_message_data = pd.DataFrame(nego_message_data)\n",
    "#df_nego_message_data\n",
    "\n",
    "sentences = df['Content'].tolist()\n",
    "labels = df['NegoOutcomeLabel'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = df_nego_message_data.Content.values\n",
    "#sentences = [\"[CLS]\"+ sentence +\" [SEP]\" for sentence in sentences]\n",
    "\n",
    "#labels= df.NegoOutcomeLabel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "#print(tokenized_texts[0])\n",
    "#print(tokenized_texts[1])\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = tokenizer(\n",
    "    sentences, max_length=512, truncation=True, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized_texts[0])\n",
    "print(tokenized_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_LEN = 512\n",
    "#input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Extrahiere input_ids und attention_mask\n",
    "input_ids = tokenized_texts[\"input_ids\"]\n",
    "attention_masks = tokenized_texts[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_masks = []\n",
    "\n",
    "#for seq in input_ids:\n",
    "#    seq_mask = [float(i>0) for i in seq]\n",
    "#    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels= train_test_split(input_ids, labels, random_state=42, test_size=0.3)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2720, 11527,  ...,     0,     0,     0],\n",
       "        [  101,  2171,  3076,  ...,     0,     0,     0],\n",
       "        [  101,  4541,  3025,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  6854,  2172,  ...,     0,     0,     0],\n",
       "        [  101,  3100,  3305,  ...,     0,     0,     0],\n",
       "        [  101,  2215, 12367,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2720, 27916,  ...,     0,     0,     0],\n",
       "        [  101,  2720,  7537,  ...,     0,     0,     0],\n",
       "        [  101,  4248,  3433,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 13676,  3988,  ...,     0,     0,     0],\n",
       "        [  101, 27916,  2116,  ...,     0,     0,     0],\n",
       "        [  101,  2748,  2228,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#je mehr desto langsamer -> batchsize immer wieder ein bisschen anpassen und ausprobieren\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler=RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler=SequentialSampler(validation_data)\n",
    "validation_dataloader =DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device Setup: Force GPU if available\n",
    "#import torch\n",
    "#from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Force CPU usage\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "#print(\"Using CPU for all computations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 2) \n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            ### Prepare data\n",
    "            input_ids, input_mask, labels = [t.to(device) for t in batch]  # Move to GPU\n",
    "        \n",
    "            outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "            loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0010 | Batch 0000/0051 | Loss: 0.7134\n",
      "training accuracy: 68.50%\n",
      "valid accuracy: 69.43%\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 0002/0010 | Batch 0000/0051 | Loss: 0.6078\n",
      "training accuracy: 68.50%\n",
      "valid accuracy: 69.43%\n",
      "Time elapsed: 2.20 min\n",
      "Epoch: 0003/0010 | Batch 0000/0051 | Loss: 0.5571\n",
      "training accuracy: 80.33%\n",
      "valid accuracy: 73.29%\n",
      "Time elapsed: 3.30 min\n",
      "Epoch: 0004/0010 | Batch 0000/0051 | Loss: 0.4360\n",
      "training accuracy: 86.03%\n",
      "valid accuracy: 72.86%\n",
      "Time elapsed: 4.41 min\n",
      "Epoch: 0005/0010 | Batch 0000/0051 | Loss: 0.4214\n",
      "training accuracy: 86.89%\n",
      "valid accuracy: 73.57%\n",
      "Time elapsed: 5.52 min\n",
      "Epoch: 0006/0010 | Batch 0000/0051 | Loss: 0.2107\n",
      "training accuracy: 95.53%\n",
      "valid accuracy: 75.71%\n",
      "Time elapsed: 6.62 min\n",
      "Epoch: 0007/0010 | Batch 0000/0051 | Loss: 0.0773\n",
      "training accuracy: 95.40%\n",
      "valid accuracy: 70.29%\n",
      "Time elapsed: 7.73 min\n",
      "Epoch: 0008/0010 | Batch 0000/0051 | Loss: 0.2947\n",
      "training accuracy: 99.02%\n",
      "valid accuracy: 74.00%\n",
      "Time elapsed: 8.84 min\n",
      "Epoch: 0009/0010 | Batch 0000/0051 | Loss: 0.0081\n",
      "training accuracy: 97.92%\n",
      "valid accuracy: 74.71%\n",
      "Time elapsed: 9.95 min\n",
      "Epoch: 0010/0010 | Batch 0000/0051 | Loss: 0.0121\n",
      "training accuracy: 99.51%\n",
      "valid accuracy: 75.71%\n",
      "Time elapsed: 11.05 min\n",
      "Total Training Time: 11.05 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        ### Prepare data\n",
    "        input_ids, input_mask, labels = [t.to(device) for t in batch]  # Move to GPU\n",
    "        \n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "    \n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### Logging\n",
    "        if not batch_idx % 250:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_dataloader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_dataloader, device):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, validation_dataloader, device):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Counts:\n",
      "Class 1: 580 predictions\n",
      "Class 0: 120 predictions\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.68      0.38      0.49       214\n",
      "     Class 1       0.77      0.92      0.84       486\n",
      "\n",
      "    accuracy                           0.76       700\n",
      "   macro avg       0.73      0.65      0.67       700\n",
      "weighted avg       0.75      0.76      0.73       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Nach dem Training:\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        ### Prepare data\n",
    "        input_ids, input_mask, labels = [t.to(device) for t in batch]\n",
    "        \n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=input_mask)\n",
    "        logits = outputs.logits  # Logits enthalten die rohen Vorhersagen des Modells\n",
    "        \n",
    "        ### Predictions und Labels sammeln\n",
    "        predictions = torch.argmax(logits, dim=1)  # Klasse mit höchster Wahrscheinlichkeit\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Count der Vorhersagen\n",
    "prediction_counts = Counter(all_predictions)\n",
    "print(\"\\nPrediction Counts:\")\n",
    "for label, count in prediction_counts.items():\n",
    "    print(f\"Class {label}: {count} predictions\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=[\"Class 0\", \"Class 1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
