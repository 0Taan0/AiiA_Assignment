{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color: orange;\"> Representation Techniques </span>**\n",
    "Loading the dataframes from the first Notebook-file to proceed with representation techniques.In this chapter, we will explore various text representation techniques that are essential for natural language processing (NLP) tasks. These techniques transform text data into numerical representations that can be used by machine learning models. The key techniques we will cover include:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the datasets from before\n",
    "import pandas as pd\n",
    "\n",
    "# List of dataset file paths\n",
    "dataset_files = ['df_autocorrected.xlsx', 'df_lowercasing.xlsx', 'df_original.xlsx', 'df_without_named_entities.xlsx', 'df_without_greetings_and_closings.xlsx']\n",
    "\n",
    "# Function to load and process each dataset\n",
    "def load_and_process_datasets(files):\n",
    "    for file in files:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"Loaded {file} with shape {df.shape}\")\n",
    "        \n",
    "        # Process the dataset (example: print first few rows)\n",
    "        print(df.head())\n",
    "        \n",
    "        # Add your processing steps here\n",
    "        # For example: data cleaning, feature extraction, etc.\n",
    "        \n",
    "        # Save the processed dataset if needed\n",
    "        processed_file = file.replace('.csv', '_processed.csv')\n",
    "        df.to_csv(processed_file, index=False)\n",
    "        print(f\"Processed dataset saved as {processed_file}\")\n",
    "\n",
    "# Load and process datasets\n",
    "load_and_process_datasets(dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color: orange;\">Bag of Words (BoW)</span>**\n",
    "While BoW is easy to implement and understand, it has some limitations, such as ignoring word order and context.\n",
    "niques. It involves the following steps:\n",
    "1. Tokenizing the text into individual words.\n",
    "2. Creating a vocabulary of all unique words in the corpus.\n",
    "3. Representing each document as a vector of word counts, where each element of the vector corresponds to the count of a specific word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color: orange;\">Term Frequency-Inverse Document Frequency (TF-IDF)</span>**\n",
    "TF-IDF is an extension of the Bag of Words model that aims to address some of its limitations. It assigns a weight to each word based on its frequency in a document (term frequency) and its rarity across the entire corpus (inverse document frequency). TF-IDF helps to highlight important words in a document while down-weighting common words that appear in many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color: orange;\">Word Embeddings</span>**\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships between them. Unlike BoW and TF-IDF, word embeddings consider the context in which words appear. Popular word embedding techniques include:\n",
    "- **Word2Vec**: Uses neural networks to learn word vectors based on their context in a large corpus.\n",
    "- **GloVe (Global Vectors for Word Representation)**: Combines the advantages of matrix factorization and context-based learning.\n",
    "- **FastText**: An extension of Word2Vec that considers subword information, making it more robust to rare and misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
